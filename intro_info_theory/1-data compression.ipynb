{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "import re\n",
    "np.random.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latex macros\n",
    "$\\renewcommand{\\vec}[1]{\\mathbf{#1}}$\n",
    "$\\renewcommand{\\ex}[1]{\\mathbb{E}\\left[{#1}\\right]}$\n",
    "$\\renewcommand{\\dkl}[2]{D_\\text{KL}\\left(\\mathbf{#1\\|#2}\\right)}$\n",
    "$\\renewcommand{\\hyp}{\\mathcal{H}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Compression\n",
    "\n",
    "## The Source Coding Theorem\n",
    "How to measure the information content of the outcome of a random experiment?\n",
    "Why do improbable outcomes convey more information than probable outcomes?\n",
    "\n",
    "Topics discussed:\n",
    "- Shannon information and Entropy\n",
    "- Motivation for the definitions being useful\n",
    "- Other measures of information: $H_0(X)$, $H_\\delta(X)$\n",
    "- Lossy and loseless compressions\n",
    "- $S_\\delta$ and the typical set $T_{N\\beta}$\n",
    "- Shannon's source coding theorem and the 'Asymptotic equipartition' principle\n",
    "\n",
    "I thought that we can start with a nice experiment, exploring compression of my Whatsapp message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(\"c://Users/ofer/projects/dl-notes/intro_info_theory/res/\")\n",
    "with open(folder/'wiki_snippet.txt', 'r') as f:\n",
    "    texts = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lotsa function defintions. names could have been better.\n",
    "\n",
    "def build_sym_table(s):\n",
    "    # more like symbol to probability table given string s\n",
    "    t = \"\".join(s)\n",
    "    sym_table = defaultdict(int)\n",
    "    for s in t:\n",
    "        sym_table[s] += 1\n",
    "    for s in sym_table:\n",
    "        sym_table[s] /= len(t)\n",
    "    return sym_table\n",
    "\n",
    "def st_N(st, N):\n",
    "    # N-sequences of symbols table\n",
    "    from itertools import product\n",
    "    prod_st = {}\n",
    "    for p in product(st.keys(), repeat=N):\n",
    "        prod_st[\"\".join(p)] = np.prod([st[k] for k in p])\n",
    "    return prod_st\n",
    "\n",
    "def delta_subset(st, delta=0.01):\n",
    "    sym = sorted(st.keys(), key=lambda x:st[x], reverse=True)\n",
    "    probs = [st[s] for s in sym]\n",
    "    i = np.searchsorted(np.cumsum(probs), 1-delta)\n",
    "    return {s:st[s] for s in sym[:i+1]}\n",
    "\n",
    "def build_encode_table(symbols):\n",
    "    n_bits = int(np.ceil(np.log2(len(symbols))))\n",
    "    et = {}\n",
    "    for i in range(len(symbols)):\n",
    "        et[symbols[i]] = f\"{i:0{n_bits}b}\" # convert to binary\n",
    "    return et\n",
    "\n",
    "def encode_str(et, s):\n",
    "    import re\n",
    "    encoded = []\n",
    "    # sweet way to split string to chunks\n",
    "    for c in re.finditer(f'.{{{len(list(et.keys())[0])}}}', s, flags=re.S):\n",
    "        try:\n",
    "            encoded.append(et[c.group()])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return \"\".join(encoded)\n",
    "\n",
    "def decode_str(dt, s):\n",
    "\n",
    "    decoded = []\n",
    "    for c in re.finditer(f'.{{{len(list(dt.keys())[0])}}}', s, flags=re.S):\n",
    "        try:\n",
    "            decoded.append(dt[c.group()])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return \"\".join(decoded)\n",
    "    \n",
    "def entropy(st):\n",
    "    prob = np.array(list(st.values()))\n",
    "    return np.sum(prob * np.log2(1/prob))\n",
    "    \n",
    "def print_errors(s, delta, N=1):\n",
    "    # visualize which part of the text gets corrupted in \n",
    "    # the encode/decode process.\n",
    "    RESET = \"\\x1b[0m\"\n",
    "    RED = \"\\x1b[1;31;43m\"\n",
    "    st = build_sym_table(s)\n",
    "    st_seq = st_N(st, N)\n",
    "    ds = delta_subset(st_seq, delta)\n",
    "    it = re.finditer(f'.{{{N}}}', s, flags=re.S)\n",
    "    print(\"\".join((c.group() if c.group() in ds else RED+c.group()+RESET for c in it )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 156527\n"
     ]
    }
   ],
   "source": [
    "st = build_sym_table(texts)\n",
    "st_2 = st_N(st, 3)\n",
    "ds = delta_subset(st_2, 0.001)\n",
    "print(len(st), len(ds))\n",
    "\n",
    "et = build_encode_table(list(st.keys()))\n",
    "dt = {v:k for k,v in et.items()}\n",
    "et_delta = build_encode_table(list(ds.keys()))\n",
    "dt_delta = {v:k for k,v in et_delta.items()}\n",
    "\n",
    "encoded_naive = encode_str(et, \"\".join(texts[:1000]))\n",
    "decoded_naive = decode_str(dt, encoded_naive)\n",
    "encoded_delta = encode_str(et_delta, \"\".join(texts[:1000]))\n",
    "decoded_delta = decode_str(dt_delta, encoded_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25340 21402\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_naive), len(encoded_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phages were discovered to be antibacterial agents and were used in the former Soviet Republic of Georgia (pioneered there by Giorgi Eliava with help from the co-discoverer of bacteriophages, Fix d'Herelle) during the 1920s and 1s for treating bacterial infections. They had widespread use, including treatment of soldiers in the Red Army. However, they were abandoned for general use in the West for several reasons:\n",
      "\n",
      "Antibiotics were discovered and marketed widely. They were easier to make, store, and to prescribe.\n",
      "Medical trials of phages were carried out, but a basic lack of understanding raised questions about the validity of these trials.[\n",
      "Publication of research in the Soviet Union was mainly in the Russian or Georgian languages and for many years, was not followed internationally.\n",
      "The use of phages has continued since the end of the Cold War in Russia,] Georgia and elsewhere in Central and Eastern Europe. The first regulated, randomized, double-blind clinical trial was reported in the Journal of Wound Care in June 20 which evaluated the safety and efficacy of a bacteriophage cocktail to treat infected venous ulcers of the leg in human patients.[20] The FDA approved the study as a Phase I clinical trial. The study's results demonstrated the safety of therapeutic application of bacteriophages, but did not show efficacy. The authors explained that the use of certain chemicals that are part of standard wound care (e.g. lactoferrin or silver) may have interfered with bacteriophage viability.[20] Shortly after that, another controlled clinical trial in Western Europe (treatment of ear infections caused by Pseudomonas aeruginosa) was reported in the journal Clinical Otolaryngology in August 9.[21] The study concludes that bacteriophage preparations were safe and effective for treatment of chronic ear infections in humans. Additionally, there have been numerous animal and other experimental clinical trials evaluating the efficacy of bacteriophages for various diseases, such as infected burns and wounds, and cystic fibrosis associated lung infections, among others.[2\n",
      "Meanwhile, bacteriophage researchers have been developing engineered viruses to overcome antibiotic resistance, and engineering the phage genes responsible for coding enzymes that degrade the biofilm matrix, phage structural proteins, and the enzymes responsible for lysis of the bacterial cell wall.[3] There have been results showing that T4 phages that are small in size and short-tailed, can be helpful in detecting E.coli in the human body.[22]\n",
      "\n",
      "Therapeutic efficacy of a phage cocktail was evaluated in a mice model with nasal infection of multidrug-resistant R) A. baumannii. Mice treated with the phage cocktail showed a 2.3-fold higher survival rate than those untreated in seven days post infection.[ In 2017 a patient with a pancreas compromised by MDR A. baumannii was put on several antibiotics, despite this the patient's health continued to deteriorate during a four-month period. Without effective antibiotics the patient was subjected to phage therapy using a phage cocktail containing nine different phages that had been demonstrated to be effective against  A. baumannii. Once on this therapy the patient's downward clinical trajectory reversed, and returned to health.[2Herelle \"quickly learned that bacteriophages are found wherever bacteria thrive: in sewers, in rivers that catch waste runoff from pipes, and in the stools of convalescent patients This includes rivers traditionally thought to have healing powers, including India's Ganges River.\n"
     ]
    }
   ],
   "source": [
    "print(decoded_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4510.9387251298895"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(st)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that if we tolerate an error rate of $\\delta=0.1$ we reduce the file to the size mandated by entropy. Of course, nobody wants every 10'th letter to be missing. Here's an illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phages were discovered to be antibacterial agents and were used in the former Soviet Republic of Georgia (pioneered there by Giorgi Eliava with help from the co-discoverer of bacteriophages, F\u001b[1;31;43mÃ©l\u001b[0mix \u001b[1;31;43md'H\u001b[0merelle) during the 1920s and 1\u001b[1;31;43m930\u001b[0ms for treating bacterial infections. They had widespread use, including treatment of soldiers in the Red Army. However, they were abandoned for general use in the West for several reason\u001b[1;31;43ms:\n",
      "\u001b[0m\n",
      "Antibiotics were discovered and marketed widely. They were easier to make, store, and to prescribe\u001b[1;31;43m.\n",
      "M\u001b[0medical trials of phages were carried out, but a basic lack of understanding raised questions about the validity of these trials.[\u001b[1;31;43m18]\u001b[0m\u001b[1;31;43m\n",
      "Pu\u001b[0mblication of research in the Soviet Union was mainly in the Russian or Georgian languages and for many years, was not followed internationall\u001b[1;31;43my.\n",
      "\u001b[0mThe use of phages has continued since the end of the Cold War in Russia,\u001b[1;31;43m[19\u001b[0m] Georgia and elsewhere in Central and Eastern Europe. The first regulated, randomized, double-blind clinical trial was reported in the Journal of Wound Care in June 20\u001b[1;31;43m09,\u001b[0m which evaluated the safety and efficacy of a bacteriophage cocktail to treat infected venous ulcers of the leg in human patients\u001b[1;31;43m.[2\u001b[0m0] The\u001b[1;31;43m FD\u001b[0mA approved the study as a Phase I clinical trial. The study's results demonstrated the safety of therapeutic application of bacteriophages, but did not show efficacy. The authors explained that the use of certain chemicals that are part of standard wound care (e.g. lactoferrin or silver) may have interfered with bacteriophage viability.[\u001b[1;31;43m20]\u001b[0m Shortly after that, another controlled clinical trial in Western Europe (treatment of ear infections caused by Pseudomonas aeruginosa) was reported in the journal Clinical Otolaryngology in August \u001b[1;31;43m200\u001b[0m\u001b[1;31;43m9.[\u001b[0m\u001b[1;31;43m21]\u001b[0m The study concludes that bacteriophage preparations were safe and effective for treatment of chronic ear infections in humans. Additionally, there have been numerous animal and other experimental clinical trials evaluating the efficacy of bacteriophages for various diseases, such as infected burns and wounds, and cystic fibrosis associated lung infections, among others\u001b[1;31;43m.[2\u001b[0m\u001b[1;31;43m1]\n",
      "\u001b[0m\n",
      "Meanwhile, bacteriophage researchers have been developing engineered viruses to overcome antibiotic resistance, and engineering the phage genes responsible for coding enzymes that degrade the biofilm matrix, phage structural proteins, and the en\u001b[1;31;43mzym\u001b[0mes responsible for lysis of the bacterial cell wall\u001b[1;31;43m.[3\u001b[0m\u001b[1;31;43m][4\u001b[0m\u001b[1;31;43m][5\u001b[0m] There have been results showing that T4 phages that are small in size and short-tailed, can be helpful in detecting E.coli in the human body.[\u001b[1;31;43m22]\u001b[0m\u001b[1;31;43m\n",
      "\n",
      "T\u001b[0mherapeutic efficacy of a phage cocktail was evaluated in a mice model with nasal infection of multidrug-resistant \u001b[1;31;43m(MD\u001b[0mR) A. baumannii. Mice treated with the phage cocktail showed a \u001b[1;31;43m2.3\u001b[0m-fold higher survival rate than those untreated in seven days post infection.[\u001b[1;31;43m23]\u001b[0m In 20\u001b[1;31;43m17 \u001b[0ma patient with a pancreas compromised by MDR A. baumannii was put on several antibiotics, despite this the patient's health continued to deteriorate during a four-month period. Without effective antibiotics the patient was subjected to phage therapy using a phage cocktail containing nine different phages that had been demonstrated to be effective against \u001b[1;31;43mMDR\u001b[0m A. baumannii. Once on this therapy the patient's downward clinical trajectory reversed, and returned to health\u001b[1;31;43m.[2\u001b[0m\u001b[1;31;43m4]\n",
      "\u001b[0m\u001b[1;31;43m\n",
      "D'\u001b[0mHerelle \"quickly learned that bacteriophages are found wherever bacteria thrive: in sewers, in rivers that catch waste runoff from pipes, and in the stools of convalescent patients\u001b[1;31;43m.\"[\u001b[0m\u001b[1;31;43m25]\u001b[0m This includes rivers traditionally thought to have healing powers, including India's Ganges River.\u001b[1;31;43m[26\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print_errors(\"\".join(texts[:10000]), 0.01, N=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was fun. Some remarks:\n",
    "- Illustration of the difference between declaritive and imperative knowledge, or, just because you can do something mathematically (encode sequence of symbols in binary) doesn't mean it's practical computationally.\n",
    "- Emojis are rare in Avnofifer, but some letters are rarer.\n",
    "- A $\\delta=0.01$ error rate is actually pretty high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to measure the information content of a random variable?\n",
    "We already defined some notions, _Shannon information content_ of $x=a_i$, $$h(x=a_i)\\equiv\\log_2\\frac1{p_i}$$\n",
    "the _entropy_ $$H(X)=\\sum_xp_i\\log_2\\frac1{p_i}$$ as the average information content of an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = np.linspace(0.0001, 0.9999)\n",
    "H = -p*np.log2(p) - (1-p)*np.log2(1-p)\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.plot(p, H, c='black')\n",
    "ax.set(title='Binary entropy function', xlabel='p', ylabel='$H_2(p)$')\n",
    "ax.set(xlim=[-0.01,1.01], ylim=[0,1.01])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this formulation of information content, why $\\log\\frac1p$? It turns out later that those numbers are related to the number of bits needed to describe the outcome of an experiment. For now though, we will see some motivating examples.\n",
    "\n",
    "Intuitivly we would want independent random variables to convey additive information, ie if $X$ and $Y$ are independent then $H(X,Y)=H(X)+H(Y)$. And indeed,\n",
    "$$\\begin{align}\n",
    "H(X,Y)&=\\sum_x\\sum_yP(x,y)\\log\\frac1{P(x,y)} \\\\&= \\sum_x\\sum_yP(x)P(y)\\log\\frac1{P(x)P(y)} \\\\\n",
    "&=\\sum_x\\sum_yP(x)P(y)\\left(\\log\\frac1{P(x)}+\\log\\frac1{P(y)}\\right) \\\\\n",
    "&=\\sum_x\\sum_yP(x)P(y)\\log\\frac1{P(x)}+\\sum_x\\sum_yP(X)P(Y)\\log\\frac1{P(y)} \\\\\n",
    "&=\\sum_xP(x)\\log\\frac1{P(x)}\\sum_yP(Y)+\\sum_yP(y)\\log\\frac1{P(y)}\\sum_xP(x) \\\\\n",
    "&=\\sum_xP(x)\\log\\frac1{P(x)}\\cdot1+\\sum_yP(y)\\log\\frac1{P(y)}\\cdot1 \\\\\n",
    "&=H(X)+H(Y)\n",
    "\\end{align}$$\n",
    "Nothing like 7 lines to prove something trivial from definitions.\n",
    "\n",
    "In the chapter's opening, the author presents the weighing problem as a motivator for questions about the nature of information. By coincidence I happened to solve it about a week before (took me ~1.5 days), and I remember coming to the conclusion that the weighings/experiment should have rule out hypotheses equally. At the time I thought of it more like a game where the adversary will choose the outcome which makes it harder for you. Here we think about the hypotheses as probabilities, and so an experiment with equal \"division\" of results is equiprobable, and we saw in the last part that when the distribution is closer to uniform we maximize the entropy, the average content.\n",
    "\n",
    "The next example is the guessing game, or a lamer version called **sixty-three**. It demonstrates the connection between the Shannon information content and the size of a file the encodes the outcomes of a random experiment. In the game, you need to guess a number between 0 and 63. 6 bits suffice to represent a number in that range, so six questions (you can formulate the question in \"human-speak\", $x\\mod2^i\\geq 2^{i-1}$). If we assume the values of $x$ are equally likely, then the answers to the questions are independent and each answer has $\\log(1/0.5)=1\\text{ bit}$. From independence we get the total Shannon information gained is always six bits. Hmmm. Also, our questioning defines a way of encoding the random variable $x$ as a binary file, write down the answers. Question, what if the number of questions depends on the number?\n",
    "\n",
    "Another example is the game of **boring battleships**. A $8\\times8$ grid, one square is a submarine. We need to guess where it is. The question is: \"how many bits can one bit convey?\" The left \"bits\" is the unit of information and the right is a binary variable. If we get the first guess right, we get $h(x=\\text{hit})=\\log64=6\\text{ bits}$, and interestingly enough, 6 bits encode the location on the grid. Somehow one bit conveyed 6 bits! If on the other hand we miss we get $h(x=\\text{miss})=\\log\\frac{64}{63}=0.0227\\text{ bits}$. Let's imagine we miss the first 32 guesses, then\n",
    "$$\\begin{align}\\log\\frac{64}{63}+\\dots+\\log\\frac{33}{32}\\\\\n",
    "=0.0227+\\dots+0.0430&=1.0\\text{ bits}\n",
    "\\end{align}$$\n",
    "We ruled out 32 hypotheses, so just like the previous game we got 1 bit. Anyway, no matter at which guess we win the information we gain always sums up to 6 bits.\n",
    "\n",
    "Lastly, we have **Wenglish**, a fake language with a 32768 long dictionary, the words are all 5 characters long and created by sampling 5 times independently from the **a...z** distribution introduced earlier. Let's imagine we are reading a Wenglish document consisting of words drawn at random. What is the information content of a word? since it's uniform we get $\\log32768=15\\text{ bits}$. So on average, 3 bits per character. \n",
    "\n",
    "What if we get the word a character at a time? It's a bit like 20 questions, we have 5 questions (what is the first/second... letter?) and we want to guess the word. Now, the letters are not uniformly distributed but if the word is $c_1c_2c_3c_4c_5$ then $c_i\\sim \\mathbf{a\\dots z}$ (techincally since the words are already sampled and constant, the actual distribution is a bit different and should be calculated straight from the sampled dictionary). Let's say $c_1=\\mathbf{z}$, what is the information content gained? $\\log(1/0.001)\\simeq10\\text{ bits}$! Contrast with $c_1=\\mathbf{a}$ where the information content is $\\log(1/0.0625)\\simeq4\\text{ bits}$. How can we make sense of that? since **z** is more rare, there are less words beginning with it, hence our \"search\" has narrowed down significantly.\n",
    "\n",
    "A question puzzles me; what if we get two **z**, isn't the information content 20 bits, more then 15 bits? isn't that some kind of contradiction? I'm not sure. It might be related to conditional information, or to the difference in the distributions caused by the sampling. In the text the author takes the 15 bit as a constraint and reasons that the rest of the letters must therefore have less information content.\n",
    "\n",
    "### Data compression\n",
    "TBD 5/15/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 4.12** How many weights do you need to weight bags of flour with integer weights $1\\leq w\\leq40$\n",
    "\n",
    "Can I do it with 40? ofcourse. How about 20? yeah, easy enough. It's a linear combination above $\\mathbb{Z_{mod2}}$, a fancy way of saying, can I achieve every number between 1-40 with each weight either contributing +,- or 0.\n",
    "\n",
    "What's the lower limit on the number of weights I'll need? if I have 3 weights, we get $3^3=27$ linear combinations, not enough to represent 40 numbers. $3^4=81$, seems enough. So at least 4 weights. Let's try solving constructivly. $w_1=1$, $w_2=2$? but then $w_2-w_1=1$ so I waste a combination. Let's choose $w_2=3$. Now I can cover 1-4. At this point I visualized the the next weight I choose has a segment around it with radius 4, covering all the numbers I could reach. So I chose $w_3=9$ to avoid overlap. Now we can cover 1-13 and with the same logic $w_4=27$ and we're done.\n",
    "\n",
    "_Comment:_ When I solved it in my head I made a mistake so it seemed at first I couldn't cover all 40 numbers. Also, I noticed that if you choose the next weight such that it's bigger then the sum of the previous, you lose all the combinations with it contributing negatively (if you don't use a heavier weight that is). How many combinations are then negative? 40, plus 1 where all the weights are missing, so we got exactly $81-(40+1)=40$ useful combinations for the 40 numbers. Nifty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.13**\n",
    "a) Is it possible to solve the 12 balls weighing problem using a sequence of three _fixed_ weighings?\n",
    "\n",
    "My initial intuition was no, since it is then \"simpler\" and therefore easier to come by. The 3 weighings can be characterized by thinking of the \"trip\" each ball takes. Each ball can start in either the left side, right side or out of the pans. Next it can change or stay. So we get $3^3=27$ possible courses for a ball. That's when I was puzzled, because it seems more than enough to identify a ball.\n",
    "\n",
    "If we take the easier problem where we know whether the fake is heavier or lighter, can we solve it? well, if we move the balls between weighings in such a way that each ball takes a unique trip than we just need to keep track in each weighing where the pan was lifted/lowered and we should be able to identify the culprit.\n",
    "\n",
    "What about the original problem? Let's imagine we have one ball that will stay on the left pan during the weighings and one ball that will stay on the right pan. Before, we could tell which one is fake because we knew the weight. Now, if the pans are so $L<R$ during the three weighings we have two possibilites: either the left is fake and its lighter, or the right is fake and it's heavier.\n",
    "\n",
    "So where's the catch? the problem is that the left ball's 'trip' is a mirror of the right ball's 'trip', ie, if we take the left ball's trip and switch every left pan postioning with a right pan positioning we'll get the right ball's trip. So without knowing the fake weight we can't differentiate between mirrored trips. Luckily though, we can find a set of 13 trips where neither is a mirror of another (I think there's some group theory justification). Well, 13 should be enough for 12 balls, right?\n",
    "\n",
    "At this point I started coding, as seen below, giving each ball a number and a trip. I ran some tests, looks good! but then, a realization... Back when I originally solved the riddle, I separated the code to different parts, the balance and the algorithm, to make sure the algorithm can't cheat. But now I did everything quickly and just programmed the balance so it would be heavier or lighter where it had the fake coin. There is one problem though; It ignores if the balance has an even number of coins! Looking at the trips I chose, it was obvious some weighings were imbalanced. I could still flip the trips though, so could I fix the situation through flipping and choosing the right trips(I had 14 candidates so I had some flexibility)? Eventually I did, and it works. With 12 balls you can identify which one's the fake and whether it's heavier or lighter. With 13, you can still tell which one's fake but not neccessarily its weight.\n",
    "\n",
    "Part b) of the exercise is to prove the general case, that with $W$ weighings you can differentiate between $(3^W-3)/2$ balls. I assume you need to somehow prove that there is a set of different unmirrored trips that even up in each weighing, like I found heuristically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['=', 'R<L', 'R<L']"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 'L'\n",
    "R = 'R'\n",
    "O = 'O'\n",
    "\n",
    "def f(tup):\n",
    "    d = {L:R, R:L, O:O}\n",
    "    return (d[tup[0]], d[tup[1]], d[tup[2]])\n",
    "\n",
    "weighings = {#1: (O,O,O),\n",
    "             2: (L,O,O),\n",
    "             3: (O,L,O),\n",
    "             4: (L,L,O),\n",
    "             5: (R,L,O),\n",
    "             6: f((O,O,L)),\n",
    "             7: f((L,O,L)),\n",
    "             8: f((R,O,L)),\n",
    "             9: f((O,L,L)),\n",
    "#              10:(L,L,L),\n",
    "             10:(R,L,L),\n",
    "             11:(O,R,L),\n",
    "             12:(L,R,L),\n",
    "             1:(R,R,L)}\n",
    "\n",
    "def weigh(weighings, n, sign='<'):\n",
    "    res = []\n",
    "    for i in range(3):\n",
    "        if weighings[n][i] == O:\n",
    "            res.append('=')\n",
    "        elif weighings[n][i] == R:\n",
    "            res.append(R+sign+L)\n",
    "        elif weighings[n][i] == L:\n",
    "            res.append(L+sign+R)\n",
    "    return res\n",
    "\n",
    "sign = '<' if np.random.randint(0, 2) else '>'\n",
    "n = np.random.randint(1, 13)\n",
    "weigh(weighings, n, sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, '<')"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.20** Sketch $f(x)=x^{x^{x^{x^{x^{.^{.^{.}}}}}}}$. _Hint:_ find the inverse function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol Codes\n",
    "\n",
    "The author states we must do exercise 2.26 before we continue.\n",
    "\n",
    "**Exercise 2.26** Prove that the relative entropy satisfies $D_\\text{KL}(P\\|Q)\\geq0$ (Gibbs' inequality) with equality if $P=Q$.\n",
    "\n",
    "Reminder: relative entropy between $P(x)$ and $Q(x)$\n",
    "$$\\dkl{P}{Q}=\\sum_xP(x)\\log\\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "Regarding the equality, if $P=Q$ then $\\log\\frac{P(x)}{Q(x)}=0$ for every $x$. Looking back to chapter 2, there was a hint there to use Jensen's inequality, $\\ex{f(\\vec{x})}\\geq f(\\ex{\\vec{x}})$ where $f$ is convex and $\\vec x$ is a random variable.\n",
    "\n",
    "We'll choose $f=-\\log$ and $\\vec x=\\frac{Q(x)}{P(x)}$. So, we have\n",
    "$$\\begin{align}\n",
    "\\sum_xP(x)\\log\\frac{P(x)}{Q(x)} &= \\sum_xP(x)-\\log\\frac{Q(x)}{F(x)} \\\\\n",
    "&= \\ex{-\\log\\frac{Q(x)}{P(x)}} \\qquad\\text{ expectation over P distribution} \\\\\n",
    "&\\geq -\\log\\ex{\\frac{Q(x)}{P(x)}} \\\\\n",
    "&=-\\log\\sum_xP(x)\\frac{Q(x)}{P(x)} \\\\\n",
    "&=-\\log\\sum_xQ(x)=-\\log1=0\n",
    "\\end{align}$$\n",
    "\n",
    "We also get the other side, if we have equality then from Jensen's inequality $\\frac{Q(x)}{P(x)}$ is a constant, so $Q(x)=P(x)$.\n",
    "Can't say I've gained some insight from proving this but, `\"We have proved it, therefore it must be true\"` or something like that.\n",
    "\n",
    "In this chapter we'll talk about lossless symbol codes, which encode one source symbol at a time. Since we want compression, some codewords will be shortened, but invariably others will be lengthened. Also, we care about practicality, so we want the codes to be easy to decode, and optimal in the compression.\n",
    "\n",
    "Formally, A **(binary) symbol code** $C$ for an ensemble $X$ is a mapping from $\\mathcal{A}_X$ to $\\{0,1\\}^+$. $c(x)$ will be the code word and $l(x)$ the length. The _extended code_ $C^+$ is obtained by concatenation without punctuation.\n",
    "\n",
    "For example, we can encode $\\mathcal{A}_X=\\{a,b,c,d\\}$ as \n",
    "\n",
    "| $a_i$ | $c(a_i)$ | $l_i$ |\n",
    "| ----- |:--------:|:-----:|\n",
    "| a | 1000 | 4 |\n",
    "| b | 0100 | 4 |\n",
    "| c | 0010 | 4 |\n",
    "| d | 0001 | 4 |\n",
    "\n",
    "What will we require from such codes? For it to be useful, we want any encoded string to have a unique decoding, and to be easy to decode.\n",
    "\n",
    "**Unique decodeablity**\n",
    "$$\\forall\\vec{x},\\vec{y}\\in\\mathcal{A}_X^+, \\vec{x}\\neq\\vec{y}\\Rightarrow c^+(\\vec{X})\\neq c^+(\\vec{Y})$$\n",
    "\n",
    "One way to have an easily decodeable code is to use prefix code, where each code is **not** a prefix of another code. That way, we can indentify the end of a codeword as soon as it arrives.\n",
    "\n",
    "Example: $C=\\{0,10,110,111\\}$ is a prefix code.\n",
    "\n",
    "**Exercise 5.8:**  is $C=\\{1,101\\}$ uniquely decodeable?  \n",
    "Yes. It is not a prefix code, but, we can identify the end of a codeword one character later. If the next character is a 1, then the last one was the codeword $1$, otherwise we get a zero and we expect $101$.\n",
    "\n",
    "**The expected length** $L(C,X)$ of a symbol code $C$ for ensemble $X$ is\n",
    "$$L(C,X)=\\sum_{x\\in\\mathcal{A}_X}P(x)l(x)$$\n",
    "\n",
    "It turns out there is a \"budget\" to how we can choose codewords if we want to retain unique decodeability. It is stated by the following:\n",
    "\n",
    "**Kraft inequality** For any uniquely decodeable code $C(X)$ over the binary alphabet $\\{0,1\\}$, the codeword lengths must satisfy:\n",
    "$$\\sum_{i=1}^I2^{-l_i}\\leq1$$\n",
    "where $I=\\left|\\mathcal{A}_X\\right|$. A uniquely decodeable code that satisfies this equality is called a _complete code_.  \n",
    "**Proof:** The full proof is in the book. I'll summarize the ideas. We will show that the quantity $S=\\left[\\sum_i2^{-l_i}\\right]$ holds than for all $N$, $S^N$ is bounded from above by a linear function, and since it is an exponential we conclude the base $S\\leq1$.\n",
    "\n",
    "In order to show that, we observe that $S^N$ is a summation of the budget of encodings of words of length N symbols, and since it is invariable to the actual encoding and only cares for the length, we can write\n",
    "$$S^N=\\sum_{l=Nl_\\text{min}}^{Nl_\\text{max}}2^{-l}\\#\\{\\text{N-words that have l-length encoding}\\}$$\n",
    "\n",
    "From unique decodeability which means one-to-one, we can deduce that $\\#\\{\\text{N to l}\\}\\leq2^l$ so\n",
    "$$S^N\\leq\\sum_{l=Nl_\\text{min}}^{Nl_\\text{max}}1\\leq Nl_\\text{max}$$\n",
    "and it is indeed bounded.\n",
    "\n",
    "**Exercise 5.14** Prove that for any set of codeword lengths $\\{l_i\\}$ satisfying the Kraft inequality, there is a prefix code having those lengths.  \n",
    "**Proof:** let $l_1,\\dots,l_n$ be lengths of codewords satisfying $\\sum_il_i\\leq1$ and $l_1\\leq l_2\\leq\\dots l_i\\leq\\dots\\leq l_n$. We will define the following codewords: for each $i$, $c_i$ will be the binary representation of the fractional part of $S_i=\\sum_{j=1}^{i-1}2^{-l_j}$.\n",
    "1. It is well defined. Since $S_i$ is a sum of numbers with finite binary representation, $S_i$ has a finite binary representation. for $i < j$, $S_i<S_j$. The binary fractional encoding can result in the same encoding if the difference between two numbers is an integer and since $\\sum_il_i\\leq1$, $c_i\\neq c_j$ ($S_n=\\sum_i^{n-1}2^{l_i}<1$ so no worries).\n",
    "2. $l(c_i)\\leq=l_i$: the binary rep of each element in $S_i$ is shorter then $l_i$, therefore the binary rep of $S_i$ is. If it is proper lesser, we can always extend by adding trailing zeros.\n",
    "3. It is a prefix code: Let's assume that for some $i<j$ we have $c_j=c_i+d$. That means that $c_j-c_i=\\underbrace{0\\dots0}_{l_i\\text{ times}}d$. From that we can deduce that $S_j-S_i<2^{-l_i}$, but we also have $S_j-S_i=\\sum_{k=i}^{j-1}2^{-l_k}\\geq2^{-l_i}$ and so, contradiction.\n",
    "\n",
    "So far we haven't been talking about probabilities, but about lengths. The problem now is to find a strategy for assigning codewords lengths in regards to the probability of a symbol, in order to minimize $L(C,X)$. How much can we minimize it? it turns out, unsurprisingly that for a uniquely decodeable code $C$\n",
    "$$H(X)\\leq L(C,X)$$\n",
    "To see that we define the implicit probabilities $q_i=2^{\\frac{-l_i}z}$ where $z=\\sum_{i'}2^{-l_{i'}}$ is a normalizing factor. Then\n",
    "$$\\begin{align}\n",
    "L(C,X) &=\\sum_ip_il_i=\\sum_ip_i\\log1/q_i-\\log z & l_i=\\log1/q_i-\\log z\\\\\n",
    "&\\geq\\sum_ip_i\\log1/p_i-\\log z & \\text{Gibbs' inequality}\\\\\n",
    "&\\geq H(X) & z\\leq1\\Rightarrow-\\log z\\geq0\n",
    "\\end{align}$$\n",
    "Equality is achieved if $z=1$ and $l_i=\\log(1/p_i)$.\n",
    "\n",
    "We are bounded by the entropy, but how close can we get?\n",
    "\n",
    "**Theorem - Source coding theorem for symbol codes** For an ensemble $X$ there exists a prefix code $C$ with expected length satisfying\n",
    "$$H(X)\\leq L(C,X)<H(X)+1$$\n",
    "\n",
    "So, the average symbol length is guaranteed to be no more than one bit above the entropy. The usefulness of this result depends on the entropy. If it is 1 bit then the gurantee is we can do no more than twice as bad, which is not a big comfort. As the entropy scales the 1 bit becomes negligible.\n",
    "\n",
    "Sometimes we can't use the best codelengths because the probabilities are not a power of two. How bad will it be? if $\\{p_i\\}$ is the probabilities and $\\{q_i\\}$ the implicit, then\n",
    "$$L(C,X)=H(X)+\\dkl{p}{q}$$\n",
    "\n",
    "Now the only a question remaining is, how do we actually find an optimal code for given probabilities $\\mathcal{P}$?  \n",
    "\n",
    "The huffman coding algorithm is a simple algorithm for finding an optimal prefix code. You iterate over two steps:\n",
    "1. Take the least two probable symbols in the alphabet. Create a tree node and have them as the left and right children. \n",
    "2. Combine their symbols to one and sum their probabilities.\n",
    "\n",
    "Since each step reduces the number of symbols by 1, we expects $\\left|\\mathcal{A}_X\\right|-1$ steps. In the end we have a binary tree, and the codewords will be the paths to the leaves.\n",
    "\n",
    "Here's an implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huffman code implementation\n",
    "        \n",
    "def huffman(ens):\n",
    "    codes = {k:\"\" for k in ens}\n",
    "    a = [(v,[k]) for k,v in ens.items()]\n",
    "    a.sort(key=lambda x:x[0])\n",
    "    for i in range(len(ens)-1):\n",
    "        l, r = a[0], a[1]\n",
    "        for s in l[1]:\n",
    "            codes[s] += \"0\"\n",
    "        for s in r[1]:\n",
    "            codes[s] += \"1\"\n",
    "        a[0] = (l[0]+r[0], l[1]+r[1])\n",
    "        a.pop(1)\n",
    "        a.sort(key=lambda x:x[0])\n",
    "    return {k:v[::-1] for k,v in codes.items()}\n",
    "\n",
    "def L(codes, ens):\n",
    "    return sum((ens[x]*len(codes[x]) for x in ens))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.16** Prove that there is no better symbol code for a source than the Huffman code.\n",
    "\n",
    "$[p_1,\\dots,p_n]$ arranged in increasing order\n",
    "from induction, $[p_1+p_2,\\dots,p_n]$ will be optimal, so\n",
    "$\\sum_{i=3}^n p_i\\log\\frac{p_i}{q_i}+(p_1+p_2)\\log\\frac{p_1+p_2}{q_t}$ is minimal\n",
    "\n",
    "$L(C,X) = L(C,X-1) + p_1+p_2$ for huffman\n",
    "\n",
    "Let's have a code of n symbols. Lets make it a code for n-1 symbols by combining the least probable symbols and choosing the shorter code for them. From the assumption $L(C`,X-1)\\geq L(C,X-1)$.\n",
    "$L(C`,X)=L(C`,X-1)+$ meh, this is the wrong approach.\n",
    "lets look at relative entropy\n",
    "\n",
    "_Spent a couple of hours yesterday, so far no strategy._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, the Huffman code is optimal. But what about in practice? How reslient is Huffman to a change in probabilities? Apparently not well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': '1', 'b': '01', 'c': '001', 'd': '000'}\n",
      "{'a': '1', 'b': '01', 'c': '001', 'd': '000'}\n",
      "{'a': '1', 'b': '01', 'c': '000', 'd': '001'}\n",
      "{'a': '1', 'b': '00', 'c': '010', 'd': '011'}\n",
      "{'a': '0', 'b': '111', 'c': '110', 'd': '10'}\n",
      "{'a': '0', 'b': '111', 'c': '110', 'd': '10'}\n",
      "{'a': '0', 'b': '111', 'c': '110', 'd': '10'}\n",
      "{'a': '11', 'b': '101', 'c': '100', 'd': '0'}\n",
      "{'a': '10', 'b': '111', 'c': '110', 'd': '0'}\n",
      "{'a': '10', 'b': '111', 'c': '110', 'd': '0'}\n",
      "{'a': '011', 'b': '00', 'c': '010', 'd': '1'}\n",
      "{'a': '011', 'b': '00', 'c': '010', 'd': '1'}\n",
      "{'a': '000', 'b': '01', 'c': '001', 'd': '1'}\n",
      "{'a': '000', 'b': '01', 'c': '001', 'd': '1'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,70,5):\n",
    "    p = i/100.\n",
    "    ens = {'a':0.7-p, 'b':0.20, 'c':0.10, 'd':p}\n",
    "#     print(ens)\n",
    "    print(huffman(ens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess the problem is there's maybe no way to reduce computation by using a previous tree?\n",
    "\n",
    "In the next chapter we will examine _arithmetic coding_ , an alternative to symbol codes which dispenses with the requirement of an integer number of bits per symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "**Exercise 5.19** Not uniquely decodeable. The string $111111$ can be interperted as three $11$ or two $111$.  \n",
    "**Exercise 5.20** Is uniquely decodeable, is prefix code.  \n",
    "**Exercise 5.21** Let's cheat!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X^1: L(C,X)=1.0000, H(X)=0.9710\n",
      "L-H=0.0290\n",
      "X^2: L(C,X)=2.0000, H(X)=1.9419\n",
      "L-H=0.0581\n",
      "X^3: L(C,X)=2.9440, H(X)=2.9129\n",
      "L-H=0.0311\n",
      "X^4: L(C,X)=3.9248, H(X)=3.8838\n",
      "L-H=0.0410\n"
     ]
    }
   ],
   "source": [
    "ens = {'0':0.6, '1':0.4}\n",
    "# ens_pow = [st_N(ens, n) for n in range(1, 5)]\n",
    "for n in range(1,5):\n",
    "    e = st_N(ens, n)\n",
    "    h = huffman(e)\n",
    "    print(f\"X^{n}: L(C,X)={L(h,e):.4f}, H(X)={entropy(e):.4f}\")\n",
    "    print(f\"L-H={L(h,e)-entropy(e):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.25** If each probability $p_i=2^{-l_i}$ then there exists a source code whose expected length equals the entropy.  \n",
    "If you define the lengths $l_i$ then the implicit probabilities $q_i$ hold that $\\dkl{p}{q}=0$ and therefore $L(C,X)=H(X)$\n",
    "\n",
    "**Exercise 5.30** The poisoned glass.\n",
    "I think there were 129 glasses, so 128 could be represent with binary and the spare had to be drank by it's own. Let's think about it with symbol coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': '100',\n",
       " '2': '101',\n",
       " '3': '010',\n",
       " '4': '011',\n",
       " '5': '000',\n",
       " '6': '001',\n",
       " '7': '1110',\n",
       " '8': '1111',\n",
       " '9': '110'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 9\n",
    "ens = {str(i):1/n for i in range(1,n+1)}\n",
    "huffman(ens)\n",
    "# L(huffman(ens), ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve on that using probability! Never ceases to amaze. Any way, as you can see above for the case of $2^3+1$, all but two hypotheses have 3-digit codewords. We can design our tests as follows: for each bit, we pour a mixture of all the drinks with that bit equal 1. If we get a positive we know the first bit is one et cetera. If the poision is in one of the glasses with 3 digit codewords, we'll know after 3 experiments.\n",
    "\n",
    "**Exercise 5.31** $\\vec{C}=c(x_1)c(x_2)\\dots$, with $X$ ensemble introduced in the beginning of the chapter and the code $C_3$. We pick a random bit from $\\vec{C}$, what is the probability it is 1?\n",
    "\n",
    "Thats actually a nice and tricky question. Tricky because it is not clear how we work with a distribution on an infinite string. It seems like using the sum rule would make our life easier, but under which events?\n",
    "\n",
    "First approach: $\\sum_{i=1}^4p(1\\mid c(a_i))p(c(a_i))$, probability of 1 given we landed on $c(a_i)$. So the conditional part is easy, but what about the prior? Obviously it is related to the probability of the letter and the length of the codeword, but not clear how to justify.\n",
    "\n",
    "Another approach was motivated by the following thought: let's imagine the code tree as kind of an automata. On each stage we stand on a node, and we wait for the next bit. What is the probability that the next one is 1? it is always $0.5$. So what if we could say, we picked a bit at random, let's think about the automata before the bit was received, what is the probability that the bit we landed on/received is 1?\n",
    "\n",
    "Anyway, this is kind of mumbo jumbo but then the formula emerged\n",
    "$$p(1)=\\sum_{i=1}^3p(1\\mid \\text{i-st digit})p(\\text{i-st digit})$$\n",
    "ie the probability of digit being one given we landed on the i-st digit of the codeword, and we see that $p(1\\mid \\text{i-st digit})$ is always $0.5$, so\n",
    "$$p(1)=\\frac12\\sum_{i=1}^3p(\\text{i-st digit})=\\frac12$$\n",
    "\n",
    "**Exercise 5.32** Just a guess, but instead of taking the lowest 2, take the lowest q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im investigating the idea of starting from assigning ceiling of information content length to each codeword and reducing from there on some greedy basis. It seems though that huffman sometimes reduces more then one bit.\n",
    "\n",
    "I think you can actually formulate this as a knapsack problem. If we start from the ceiling values. Each bit we reduce decreases $L(C,X)$ but costs us $2^{l_i}$ from our budget. It's a bit trickier since we get access to later bits only by choosing earlier ones, but since their value/price always diminishes every optimal solution will be consistent in that regard. This doesn't help much since knapsack is NP-hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im investigating the idea of starting from assigning ceiling of information content length to each codeword and reducing from there on some greedy basis. It seems though that huffman sometimes reduces more then one bit.\n",
    "\n",
    "I think you can actually formulate this as a knapsack problem. If we start from the ceiling values. Each bit we reduce decreases $L(C,X)$ but costs us $2^{l_i}$ from our budget. It's a bit trickier since we get access to later bits only by choosing earlier ones, but since their value/price always diminishes every optimal solution will be consistent in that regard. This doesn't help much since knapsack is NP-hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  0.1928   2.4  3  2  1.5424 0.1028\n",
      "e  0.0913   3.5  4  4  1.4608 0.0974\n",
      "t  0.0706   3.8  4  4  1.1296 0.0753\n",
      "o  0.0689   3.9  4  4  1.1024 0.0735\n",
      "i  0.0599   4.1  5  4  1.9168 0.0618\n",
      "n  0.0596   4.1  5  4  1.9072 0.0615\n",
      "a  0.0575   4.1  5  4  1.8400 0.0594\n",
      "s  0.0567   4.1  5  4  1.8144 0.0585\n",
      "r  0.0508   4.3  5  5  1.6256 0.0524\n",
      "l  0.0335   4.9  5  5  1.0720 0.0346\n",
      "u  0.0334   4.9  5  5  1.0688 0.0345\n",
      "h  0.0313   5.0  5  5  1.0016 0.0668\n",
      "d  0.0285   5.1  6  5  1.8240 0.0588\n",
      "c  0.0263   5.2  6  5  1.6832 0.0543\n",
      "m  0.0235   5.4  6  6  1.5040 0.0485\n",
      "p  0.0192   5.7  6  6  1.2288 0.0396\n",
      "f  0.0173   5.9  6  6  1.1072 0.0357\n",
      "y  0.0164   5.9  6  6  1.0496 0.0339\n",
      "g  0.0133   6.2  7  6  1.7024 0.0270\n",
      "b  0.0128   6.3  7  6  1.6384 0.0260\n",
      "w  0.0119   6.4  7  7  1.5232 0.0242\n",
      "k  0.0084   6.9  7  7  1.0752 0.0171\n",
      "x  0.0073   7.1  8  7  1.8688 0.0147\n",
      "v  0.0069   7.2  8  8  1.7664 0.0139\n",
      "q  0.0008  10.3 11  9  1.6384 0.0016\n",
      "z  0.0007  10.5 11 10  1.4336 0.0014\n",
      "j  0.0006  10.7 11 10  1.2288 0.0012\n",
      "(9, 0.49951171875)\n",
      "4.146199999999999 1.0\n",
      "4.6638 0.69677734375\n"
     ]
    }
   ],
   "source": [
    "abc = {\n",
    "    'a':0.0575,\n",
    "    'b':0.0128,\n",
    "    'c':0.0263,\n",
    "    'd':0.0285,\n",
    "    'e':0.0913,\n",
    "    'f':0.0173,\n",
    "    'g':0.0133,\n",
    "    'h':0.0313,\n",
    "    'i':0.0599,\n",
    "    'j':0.0006,\n",
    "    'k':0.0084,\n",
    "    'l':0.0335,\n",
    "    'm':0.0235,\n",
    "    'n':0.0596,\n",
    "    'o':0.0689,\n",
    "    'p':0.0192,\n",
    "    'q':0.0008,\n",
    "    'r':0.0508,\n",
    "    's':0.0567,\n",
    "    't':0.0706,\n",
    "    'u':0.0334,\n",
    "    'v':0.0069,\n",
    "    'w':0.0119,\n",
    "    'x':0.0073,\n",
    "    'y':0.0164,\n",
    "    'z':0.0007,\n",
    "    '-':0.1928\n",
    "}\n",
    "h_abc = huffman(abc)\n",
    "temp = {}\n",
    "# sorted(abc.keys(), key=lambda x:abc[x], reverse=True)\n",
    "def max_short(l, budget):\n",
    "    bits = 0\n",
    "    cost = 2.**-l\n",
    "    while(cost+budget <= 1. and bits < l):\n",
    "        bits += 1\n",
    "        cost += 2.**-(l-bits)\n",
    "    return bits, cost\n",
    "def budget(c):\n",
    "    return sum((2**-len(code) for code in c.values()))\n",
    "def u(x):\n",
    "    ic = np.log2(1/abc[x])\n",
    "    ce = int(np.ceil(ic))\n",
    "    bits, cost = max_short(ce, budget(temp))\n",
    "    if cost == 0:\n",
    "        return 0\n",
    "    return abc[x]/cost\n",
    "for k in sorted(abc.keys(), key=u, reverse=True):\n",
    "    ic = np.log2(1/abc[k])\n",
    "    ce = int(np.ceil(ic))\n",
    "    temp[k] = '0'*ce\n",
    "    \n",
    "    print(f\"{k}  {abc[k]}  {ic:4.1f} {ce:2} {len(h_abc[k]):2}  {abc[k]/2**(-ce):.4f} {u(k):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Codes\n",
    "\n",
    "Why the name stream codes? I believe that the encoding and decoding can be done on a stream, ie the file doesn't have to be examined in advance.\n",
    "\n",
    "Two codes are introduced in this chapter, _Arithmetic coding_ and _Lempel-Ziv coding_. They both solve the 1-bit symbol encoding overhead of the Huffman coding, by seamlessly encoding longer strings.\n",
    "\n",
    "Implementation of arithmetic codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binfrac(s):\n",
    "    if s == '':\n",
    "        return 0\n",
    "    return int(s, base=2) / (1 << len(s))\n",
    "\n",
    "def encode_arith(x, sym, pd):\n",
    "    u = 0.\n",
    "    v = 1.\n",
    "    p = v - u\n",
    "    enc = ''\n",
    "    for n in range(len(x)-1):\n",
    "        a_i = x[n]\n",
    "        Q = sum((pd(a, x[:n]) for a in sym[:sym.index(a_i)]))\n",
    "        R = Q + pd(a_i, x[:n])\n",
    "        v = u + p*R\n",
    "        u = u + p*Q\n",
    "        p = v - u\n",
    "#         while binfrac(enc) < u:\n",
    "#             print(u, v, binfrac(enc), binfrac(enc+'1'), enc)\n",
    "#             can_add_1 =  binfrac(enc+'1') < v\n",
    "#             if not can_add_1:\n",
    "#                 enc += '0'\n",
    "#             elif u <= binfrac(enc+'01') < v:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 enc += '1'\n",
    "    while binfrac(enc) < u:\n",
    "        print(u, v, binfrac(enc), binfrac(enc+'1'), enc)\n",
    "        can_add_1 =  binfrac(enc+'1') < v\n",
    "        if not can_add_1:\n",
    "            enc += '0'\n",
    "        else:\n",
    "            enc += '1'\n",
    "    return enc, u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.59659375 0.6226940625 0 0.5 \n",
      "0.59659375 0.6226940625 0.5 0.75 1\n",
      "0.59659375 0.6226940625 0.5 0.625 10\n",
      "0.59659375 0.6226940625 0.5 0.5625 100\n",
      "0.59659375 0.6226940625 0.5625 0.59375 1001\n",
      "0.59659375 0.6226940625 0.59375 0.609375 10011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('100111', 0.59659375, 0.6226940625)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens = {'a': 0.425, 'b': 0.425, 'e': 0.15}\n",
    "def p_bent(s, cond):\n",
    "    # modeled using rule of succession\n",
    "    # https://en.wikipedia.org/wiki/Rule_of_succession\n",
    "    if 'e' in cond:\n",
    "        return 1. if s == 'e' else 0.\n",
    "    if s == 'e':\n",
    "        return ens[s]\n",
    "    if cond == \"\":\n",
    "        return ens[s]\n",
    "    p_a = (cond.count('a') + 1) / (cond.count('a') + cond.count('b') + 2)\n",
    "    return (p_a if s == 'a' else 1 - p_a) * .85\n",
    "encode_arith(\"bbbae\", list(ens.keys()), p_bent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619140625 0.62109375\n",
      "0.62109375 0.6220703125\n"
     ]
    }
   ],
   "source": [
    "print(binfrac('100111101'), binfrac('10011111'))\n",
    "print(binfrac('1001111100'), binfrac('1001111101'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay I'll stop for now. Surprisingly tricky, this took me two hours so far. The encoding is a bit confusing, ie how to decide whether a digit is certain. Also when terminating, my approach failed and cost an extra bit...\n",
    "\n",
    "**Exercise 6.1** Show the overhead required to terminate a message is never more than 2 bits, relative to the ideal message length given the probabilistic model $\\mathcal{H}$, $h(\\vec x\\mid\\mathcal{H})=\\log\\left[1/P(\\vec x\\mid\\mathcal{H}\\right)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(\\vec x\\mid\\hyp)=\\prod_i^NP(x_i\\mid x_1\\dots x_{i-1},\\hyp)$, so\n",
    "$$h(\\vec x\\mid\\hyp)=\\sum\\log\\left[1/P(x_i\\mid x_1\\dots x_{i-1},\\hyp)\\right]$$\n",
    "\n",
    "My head isn't working today...\n",
    "\n",
    "In general if $I$ is an interval of width $w$,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is arithmetic coding? Let's consider a binary file. If we think of the bits as the fractional part of a binary representation, we can think of the file as an interval contained in the interval $[0, 1)$.\n",
    "\n",
    "For example, the string $01101$ defines the interval $[0.01101, 0.01110)$ (in binary). The string $01110$ defines the interval $[0.01110, 0.01111)$. Now, let's say we have an ensemble and a distribution, we could divide the interval $[0,1)$ so that each subinterval corresponds to the probability of a symbol. So, for symbol $a_i$, subinterval $I_i$ will hold\n",
    "$$\\left|I_i\\right| = p(a_i)$$. Next, for the pair $a_ia_j$, we can divide the interval $I_i$ according to the distribution $p(a\\mid a_i)$ so\n",
    "$$\\left|I_{ij}\\right| = p(a_j\\mid a_i)*\\left|I_i\\right|=p(a_j\\mid a_i)*p(a_i)=p(a_j,a_i)$$\n",
    "and so forth. The intuition is that bigger intervals will require less binary digits to represent and with our modeling bigger intervals will correspond to more likely sequences. The encoding will be a binary string whose interval lies within the sequences interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes for Integers\n",
    "\n",
    "TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
